---
layout: post
title: "「ゼロから作る Deep Learning」で機械学習を学ぶ その02 学習まで"
tags : [Python, Deep Learning]
date: 2025-07-22 19:36:44
---

今回は第4章「ニューラルネットワークの学習」

ソースは下記
[ryotakato/de_zero](https://github.com/ryotakato/de_zero)

前回までは推論処理だけだったのが、
この章で訓練データを元にネットワークが学習を行い、自己を改善していく様子が見て取れる。

ネットワークの評価のための損失関数に、交差エントロピー誤差、
その値の勾配を求めるために、偏微分を使った勾配降下法
を用い、10000回の学習を行って、だんだんと改善していく。

実際に作ってみたが、
最初は全然学習してくれなくて、おかしいなと思っていたら、
最初の層の重みが何度やっても勾配が0のままで、
そりゃ改善も何もないよなって状況だった。

問題だったのは、common/functions.pyの、numerical_gradient関数で、
これ書籍では、for文でlen(x)回までループを回すんだけど、
これが模範ソースではnp.nditerにてイテレータを取得するようになっていた。
これにより、重みなどが2次元以上の配列の場合でも、一個一個の要素毎にループ内の処理を実行するようにできる。
（for文のパターンだと、外側の配列分しかループしない。それにより変数の参照渡しにより、期待したような偏微分を計算していなかった）

そこを模範ソースの通りに改善したら学習していきそうだったので回してみて、損失関数の値をプロットしてみた。
なお、1世代評価するのに、10秒ぐらいかかるので、10000回は無理で、1000回だけにした。
これでも損失関数の値が下がっていって、だんだんと学習していく様子が分かると思う。


![learning_deep_learning_from_zero_02_01]({{ BASE_PATH }}/images/2025/07/22/learning_deep_learning_from_zero_02_01.png)



確かに1世代あたり10秒は長いなー。
次の5章では、今回の誤差逆伝播法を使って勾配法をもっと早くしていくみたいなので、
それができたら10000回を回して結果を出してみたい。


